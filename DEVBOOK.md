# DEVBOOK : Chatbot intelligent pour techniciens du b√¢timent

## 1. Introduction

Ce document d√©crit en d√©tail la conception et la mise en ≈ìuvre d'un chatbot intelligent destin√© aux techniciens du b√¢timent. L'objectif est de fournir aux techniciens un assistant num√©rique capable de r√©pondre √† des questions en langage naturel sur les √©quipements de b√¢timent et leurs interventions de maintenance.

Le chatbot s'appuie sur des donn√©es internes (fichiers CSV listant les √©quipements et les interventions) et utilise un mod√®le de langage avanc√© pour formuler des r√©ponses claires et pertinentes.

### Contexte

Les techniciens du b√¢timent doivent souvent consulter rapidement des informations techniques (caract√©ristiques d'un √©quipement, historique des interventions, proc√©dures, etc.). Ce chatbot offre une interface conversationnelle simple o√π le technicien pose ses questions et obtient des r√©ponses imm√©diates, sans avoir √† parcourir manuellement des documents techniques ou bases de donn√©es.

### Fonctionnalit√©s cl√©s

- **Consultation des √©quipements** : interroger la base des √©quipements (par exemple conna√Ætre les caract√©ristiques d'un appareil ou son identifiant).
- **Consultation des interventions** : retrouver l'historique ou la date de la derni√®re intervention de maintenance sur un √©quipement donn√©.
- **R√©ponses en langage naturel** : le technicien pose des questions libres, le chatbot comprend la demande et y r√©pond de fa√ßon contextuelle.
- **Mod√®le de langage local** : utilisation de Mistral 7B (un mod√®le de langage de Mistral AI) ex√©cut√© localement via Ollama, garantissant la confidentialit√© des donn√©es et une disponibilit√© sans d√©pendance √† Internet.
- **Int√©gration des donn√©es internes** : le chatbot puise dans les fichiers CSV fournis (√©quipements et interventions) comme base de connaissances, pour fournir des r√©ponses pr√©cises bas√©es sur les donn√©es r√©elles de l'entreprise.

En r√©sum√©, ce DEVBOOK sert de r√©f√©rence technique compl√®te pour d√©velopper ce chatbot, en couvrant son architecture, les choix technologiques, le code du backend et du frontend, ainsi que les instructions de d√©ploiement et des pistes d'am√©lioration.

## 2. Architecture

L'architecture du syst√®me se d√©compose en plusieurs composants interconnect√©s. Chaque composant a un r√¥le d√©fini, et l'ensemble forme une application chatbot coh√©rente :

- **Frontend (Next.js)** : Une interface web utilisateur o√π le technicien peut saisir ses questions et voir les r√©ponses. Next.js est utilis√© pour cr√©er une application React moderne, offrant une zone de texte pour la question et une zone d'affichage pour la r√©ponse du chatbot.

- **Backend (FastAPI)** : Un service web en Python qui sert d'interm√©diaire entre le frontend, les donn√©es, et le mod√®le d'IA. Il expose des API REST permettant d'une part de requ√™ter les donn√©es (√©quipements, interventions) et d'autre part d'obtenir une r√©ponse du chatbot. C'est le c≈ìur logique de l'application.

- **Mod√®le de langage (Mistral + Ollama)** : Le moteur d'IA qui g√©n√®re les r√©ponses en langage naturel. Mistral 7B est le mod√®le de langage utilis√©, d√©ploy√© localement √† l'aide d'Ollama. Ollama agit comme serveur d'inf√©rence local : il h√©berge le mod√®le Mistral et offre une API locale (HTTP) pour g√©n√©rer des r√©ponses √† partir de ce mod√®le.

- **Base de connaissances (CSV)** : Les donn√©es sur lesquelles le chatbot s'appuie ‚Äì principalement deux fichiers CSV:

  - `equipements.csv` listant les √©quipements du b√¢timent (avec des colonnes telles que identifiant, nom, type, localisation, date d'installation, etc.).
  - `interventions.csv` listant les interventions de maintenance (avec des colonnes comme identifiant de l'intervention, date, identifiant de l'√©quipement concern√©, description de l'intervention, technicien responsable, etc.).

  Ces fichiers font office de base de donn√©es simplifi√©e. Le backend les charge en m√©moire et peut effectuer des recherches dans ces donn√©es pour trouver l'information pertinente √† la question pos√©e.

### Sch√©ma global d'interaction

Le technicien interagit avec l'interface Next.js (frontend), qui envoie la question au backend FastAPI. Le backend analyse la question : si n√©cessaire, il interroge les donn√©es CSV (par exemple pour trouver un √©quipement sp√©cifique ou l'historique des interventions correspondantes).

Le backend construit ensuite une requ√™te vers le mod√®le de langage Mistral (via l'API d'Ollama) en lui fournissant la question de l'utilisateur et √©ventuellement des informations contextuelles extraites des CSV. Le mod√®le g√©n√®re une r√©ponse en langage naturel, que le backend renvoie au frontend. Enfin, l'interface Next.js affiche cette r√©ponse au technicien.

Le sch√©ma suivant illustre ce flux :

```
Utilisateur (Technicien)
       ‚Üì (Question en langage naturel)
[ Frontend Next.js ] ‚Äî‚Üí (requ√™te HTTP) ‚Äî‚Üí [ Backend FastAPI ]
       ‚ÜòÔ∏é                           ‚ÜôÔ∏é
        [ Fichiers CSV (Donn√©es) ]    [ Mod√®le Mistral via Ollama ]
```

Gr√¢ce √† cette architecture modulaire, chaque composant peut √™tre d√©velopp√© et test√© ind√©pendamment : l'interface utilisateur, l'API backend, le mod√®le d'IA, et la couche de donn√©es.

## 3. Backend (FastAPI)

Le backend est d√©velopp√© en Python en utilisant FastAPI, un framework web rapide et moderne bien adapt√© pour cr√©er des APIs REST. Il remplit plusieurs fonctions : charger les donn√©es CSV, exposer des endpoints API pour acc√©der √† ces donn√©es, et int√©grer le mod√®le de langage pour g√©n√©rer les r√©ponses du chatbot.

### Chargement des CSV avec Pandas

Au d√©marrage du backend, les fichiers CSV contenant les donn√©es d'√©quipements et d'interventions sont charg√©s en m√©moire. Pour cela, on utilise la biblioth√®que Pandas, qui permet de lire des fichiers CSV et de manipuler les donn√©es sous forme de DataFrame (tableaux structur√©s).

```python
import pandas as pd
from fastapi import FastAPI, HTTPException

# Chargement des donn√©es CSV dans des DataFrame pandas
try:
    df_equip = pd.read_csv("equipements.csv")  # Donn√©es des √©quipements
    df_int = pd.read_csv("interventions.csv")  # Donn√©es des interventions
except FileNotFoundError as e:
    raise RuntimeError(f"Erreur: fichier non trouv√© - {e}")

# Optionnel: on peut v√©rifier quelques entr√©es
print(f"{len(df_equip)} √©quipements charg√©s, {len(df_int)} interventions charg√©es.")

app = FastAPI(title="Chatbot Technicien API")
```

Ce chargement initial permet d'avoir les donn√©es pr√™tes pour les requ√™tes API sans recharger les fichiers √† chaque appel (am√©liorant les performances). Si les fichiers sont volumineux, cette approche reste viable tant que la taille tient en m√©moire, sinon il faudrait envisager une base de donn√©es ou un chargement partiel.

### API pour requ√™ter les √©quipements et interventions

Le backend expose des endpoints permettant de consulter les donn√©es brutes sur les √©quipements et interventions. Ces endpoints fournissent une sorte d'acc√®s "lecture seule" √† la base de connaissances CSV, utile pour des tests ou pour des fonctionnalit√©s compl√©mentaires.

Deux endpoints principaux sont d√©finis :

- `GET /equipment/{id}` ‚Äì R√©cup√©rer les informations d'un √©quipement donn√© par son identifiant (unique).
- `GET /equipment/{id}/interventions` ‚Äì Obtenir la liste des interventions li√©es √† un √©quipement sp√©cifique.

```python
from typing import List

# Endpoint 1: Obtenir les informations d'un √©quipement par son ID
@app.get("/equipment/{equip_id}")
def get_equipment(equip_id: int):
    # Filtrer le DataFrame des √©quipements sur l'identifiant
    equip_data = df_equip[df_equip["id"] == equip_id]
    if equip_data.empty:
        raise HTTPException(status_code=404, detail="√âquipement non trouv√©")
    # Convertir le r√©sultat en dictionnaire (pour s√©rialisation JSON)
    record = equip_data.to_dict(orient="records")[0]
    return {"equipment": record}

# Endpoint 2: Obtenir la liste des interventions pour un √©quipement donn√©
@app.get("/equipment/{equip_id}/interventions")
def get_interventions_for_equipment(equip_id: int):
    # V√©rifier d'abord que l'√©quipement existe
    equip_data = df_equip[df_equip["id"] == equip_id]
    if equip_data.empty:
        raise HTTPException(status_code=404, detail="√âquipement non trouv√©")
    # Filtrer les interventions li√©es √† cet √©quipement
    related_int = df_int[df_int["equip_id"] == equip_id]
    # Convertir en liste de dict (liste d'interventions)
    interventions_list = related_int.to_dict(orient="records")
    return {"equipment_id": equip_id, "interventions": interventions_list}
```

Ces endpoints permettent par exemple √† un client (ou au frontend) de r√©cup√©rer les donn√©es brutes. Exemples de requ√™tes possibles :

- `GET /equipment/42` pourrait retourner les d√©tails de l'√©quipement d'ID 42 (par ex. son nom, type, etc.).
- `GET /equipment/42/interventions` retournerait toutes les interventions li√©es √† l'√©quipement 42 (par ex. dates et descriptions des maintenances).

### Int√©gration d'Ollama et du mod√®le Mistral AI

Le c≈ìur ¬´ intelligent ¬ª du backend r√©side dans l'appel au mod√®le de langage pour g√©n√©rer une r√©ponse √† partir de la question de l'utilisateur et des donn√©es contextuelles. Nous utilisons ici Mistral 7B, un mod√®le de langage d√©velopp√© par Mistral AI, que nous faisons tourner localement via Ollama.

Avant de pouvoir interroger Mistral, il faut s'assurer que Ollama est install√© et que le mod√®le Mistral est disponible :

1. Installer Ollama sur la machine
2. T√©l√©charger le mod√®le Mistral 7B via Ollama: `ollama pull mistral`
3. Lancer Ollama en s'assurant qu'il h√©berge le mod√®le (par d√©faut sur http://localhost:11434)

D√©finissons d'abord le mod√®le de donn√©es attendu en entr√©e avec Pydantic :

```python
from pydantic import BaseModel

class Question(BaseModel):
    question: str
```

Maintenant, le endpoint du chatbot lui-m√™me :

```python
import requests

@app.post("/ask")
def ask_question(payload: Question):
    user_question = payload.question  # La question pos√©e en texte naturel

    # 1. (Optionnel) Recherche de contexte dans les donn√©es internes
    contexte = ""
    # Par exemple, si la question contient un identifiant ou nom d'√©quipement, on peut extraire des infos
    for _, equip in df_equip.iterrows():
        name = str(equip.get("nom") or equip.get("name") or "")  # supposons colonne nom/name
        if name and name.lower() in user_question.lower():
            # On a trouv√© un √©quipement dont le nom est mentionn√© dans la question
            equip_id = equip["id"]
            # R√©cup√©rer les interventions associ√©es
            related_int = df_int[df_int["equip_id"] == equip_id]
            last_int_date = None
            if not related_int.empty:
                # trouver la derni√®re intervention (par date la plus r√©cente)
                related_int = related_int.sort_values(by="date", ascending=False)
                last_int_date = related_int.iloc[0]["date"]
            # Construire une phrase de contexte avec ces infos
            contexte = f"√âquipement {name} (ID {equip_id}). "
            if last_int_date:
                contexte += f"Derni√®re intervention le {last_int_date}. "
            contexte += "\n"
            break  # on prend le premier √©quipement correspondant pour le contexte

    # 2. Pr√©paration du prompt pour le mod√®le Mistral
    if contexte:
        prompt = f"Contexte:\n{contexte}\nQuestion:\n{user_question}\nR√©ponse:"
    else:
        prompt = f"Question:\n{user_question}\nR√©ponse:"

    # 3. Appel au mod√®le Mistral via l'API d'Ollama
    ollama_url = "http://localhost:11434/api/generate"
    data = {
        "model": "mistral",   # nom du mod√®le tel que connu par Ollama
        "prompt": prompt,
        "stream": False       # on demande une r√©ponse compl√®te en une fois
    }
    try:
        res = requests.post(ollama_url, json=data, timeout=30)
    except requests.RequestException as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de l'appel au mod√®le: {e}")
    if res.status_code != 200:
        raise HTTPException(status_code=500, detail=f"Mod√®le IA a retourn√© une erreur {res.status_code}")
    # Analyser la r√©ponse du mod√®le
    result_json = res.json()
    answer_text = result_json.get("response") or ""  # le texte de la r√©ponse g√©n√©r√©e

    # 4. Renvoi de la r√©ponse sous forme JSON
    return {"answer": answer_text.strip()}
```

Avec ce endpoint en place, le backend est capable de recevoir une question et de retourner une r√©ponse g√©n√©r√©e par le mod√®le en tenant compte des donn√©es d'√©quipement. En quelque sorte, on a impl√©ment√© une forme simplifi√©e de RAG (Retrieval-Augmented Generation) o√π l'on fournit au mod√®le des informations suppl√©mentaires issues d'une base de connaissances structur√©e.

## 4. Frontend (Next.js)

Le frontend de notre application est une interface web simple d√©velopp√©e avec Next.js, framework React. Il offre une page unique permettant au technicien d'interagir avec le chatbot. L'accent est mis sur la simplicit√© d'utilisation : une zone de texte pour poser la question et une zone d'affichage de la r√©ponse.

### Interface utilisateur

L'interface consiste en :

- Un champ de saisie (textarea) o√π l'utilisateur entre sa question
- Un bouton d'envoi pour d√©clencher la requ√™te
- Une zone d'affichage de la r√©ponse du chatbot

### Exemple de code Next.js (React)

```jsx
// pages/index.js (ou app/page.jsx dans Next 13+ avec app router)
import { useState } from "react";

export default function ChatbotPage() {
  const [question, setQuestion] = useState("");
  const [answer, setAnswer] = useState("");
  const [loading, setLoading] = useState(false);

  const handleAsk = async () => {
    if (!question.trim()) return; // ne rien faire si la question est vide
    setLoading(true);
    setAnswer(""); // on r√©initialise la r√©ponse affich√©e
    try {
      const res = await fetch("http://localhost:8000/ask", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ question }),
      });
      if (!res.ok) {
        throw new Error(`Erreur ${res.status}`);
      }
      const data = await res.json();
      setAnswer(data.answer);
    } catch (err) {
      console.error("Erreur lors de la requ√™te:", err);
      setAnswer("Une erreur est survenue. Veuillez r√©essayer.");
    } finally {
      setLoading(false);
    }
  };

  // Permet d'envoyer la question avec Entr√©e
  const handleKeyDown = (e) => {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      handleAsk();
    }
  };

  return (
    <div
      style={{ maxWidth: 600, margin: "2rem auto", fontFamily: "sans-serif" }}
    >
      <h1>ü§ñ Assistant Technicien du B√¢timent</h1>
      <textarea
        rows={4}
        value={question}
        onChange={(e) => setQuestion(e.target.value)}
        onKeyDown={handleKeyDown}
        placeholder="Posez une question sur un √©quipement ou une intervention..."
        style={{ width: "100%", padding: "0.5rem" }}
      />
      <button
        onClick={handleAsk}
        disabled={loading}
        style={{ marginTop: "0.5rem", padding: "0.5rem 1rem" }}
      >
        {loading ? "Recherche..." : "Envoyer"}
      </button>
      <div style={{ marginTop: "1rem", whiteSpace: "pre-wrap" }}>
        {loading ? (
          <p>R√©ponse en cours...</p>
        ) : (
          <p>
            <strong>R√©ponse: </strong>
            {answer}
          </p>
        )}
      </div>
    </div>
  );
}
```

## 5. D√©ploiement et Tests

### Pr√©-requis et configuration

1. **Mod√®le Mistral via Ollama** :

   - Installer Ollama sur la machine de production
   - T√©l√©charger le mod√®le: `ollama pull mistral`
   - Lancer le service Ollama

2. **Backend FastAPI** :

   - Pr√©parer un environnement Python avec les d√©pendances n√©cessaires
   - Fichier requirements.txt: `fastapi`, `uvicorn`, `pandas`, `requests`
   - Lancer avec: `uvicorn main:app --reload --port 8000` (dev) ou sans `--reload` en production

3. **Frontend Next.js** :

   - Installer les d√©pendances Node (`npm install` ou `yarn`)
   - Configurer l'URL du backend (variable d'environnement `NEXT_PUBLIC_API_URL`)
   - Lancer avec: `npm run dev` (dev) ou `npm run build && npm run start` (prod)

4. **Communication entre frontend et backend** :
   - Configurer CORS si n√©cessaire:

```python
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### Tests manuels de l'API backend

FastAPI fournit une interface Swagger accessible √† `/docs` (ex: http://localhost:8000/docs) pour tester les endpoints:

- `GET /equipment/1` - infos d'un √©quipement
- `GET /equipment/1/interventions` - liste des interventions
- `POST /ask` avec JSON `{"question": "Ma question?"}` - test du chatbot

Exemple avec curl:

```bash
curl -X POST "http://localhost:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "Quand a eu lieu la derni√®re intervention sur le G√©n√©rateur A ?"}'
```

### Bonnes pratiques de d√©ploiement

- **Journalisation** : Logguer les interactions importantes c√¥t√© backend
- **Surveillance** : Surveiller l'utilisation CPU/M√©moire du mod√®le Mistral
- **S√©curit√©** : Valider/limiter la taille des questions re√ßues
- **Dockerisation** (optionnel) : Faciliter le d√©ploiement avec Docker
- **Documentation utilisateur** : R√©diger un guide pour les techniciens

## 6. Am√©liorations possibles

Plusieurs am√©liorations peuvent √™tre envisag√©es:

1. **Reconnaissance vocale** : Int√©grer un syst√®me STT pour que les techniciens puissent parler leurs questions

2. **Suggestions automatiques** : Proposer des questions types ou compl√©ter la question en cours de frappe

3. **Conservation du contexte conversationnel** : Garder l'historique des √©changes pour permettre des conversations plus naturelles

4. **Enrichissement de la base de connaissances** : Int√©grer un moteur d'indexation pour interroger des documents plus divers (manuels PDF, sch√©mas, etc.)

5. **Interface utilisateur am√©lior√©e** : Affiner le design pour une meilleure UX

6. **Multi-langue** : Permettre au chatbot de fonctionner en plusieurs langues

En conclusion, ce DEVBOOK a pr√©sent√© une solution compl√®te pour un chatbot technique, depuis l'architecture jusqu'au code et au d√©ploiement. Cette base peut √™tre enrichie avec les suggestions ci-dessus pour aller plus loin. En appliquant ces principes et en tirant parti de la puissance du mod√®le Mistral AI, les techniciens du b√¢timent disposeront d'un outil innovant pour acc√©der rapidement √† l'information et gagner en efficacit√© dans leurs t√¢ches quotidiennes.
